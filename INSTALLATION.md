# Installation des AmÃ©liorations NomosX v2.0

## âš ï¸ Ã‰tapes Critiques

L'installation automatique a Ã©chouÃ© Ã  cause de permissions Windows. Suivez ces Ã©tapes **dans l'ordre** :

### 1. ArrÃªter le serveur de dÃ©veloppement

```bash
# Dans le terminal oÃ¹ npm run dev tourne
Ctrl+C
```

### 2. Installer les nouvelles dÃ©pendances

```bash
npm install @sentry/nextjs @anthropic-ai/sdk ai
```

**Note**: Si l'installation Ã©choue encore avec EPERM:
1. Fermez VS Code/Cursor complÃ¨tement
2. RedÃ©marrez en tant qu'Administrateur
3. Relancez la commande

### 3. Configurer les variables d'environnement

Ã‰ditez `.env` et ajoutez :

```bash
# Redis Cache (optionnel mais recommandÃ©)
# Option 1: Local avec Docker
REDIS_URL=redis://localhost:6379

# Option 2: Upstash (cloud serverless gratuit)
# CrÃ©er compte sur https://upstash.com
# REDIS_URL=redis://default:xxx@xxx.upstash.io:6379

# Anthropic Claude (optionnel - fallback LLM)
# Get key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-xxx

# Sentry (optionnel - production monitoring)
# Get DSN at: https://sentry.io/
SENTRY_DSN=https://xxx@sentry.io/xxx
SENTRY_ORG=nomosx
SENTRY_PROJECT=nomosx-production
NEXT_PUBLIC_SENTRY_DSN=https://xxx@sentry.io/xxx
```

### 4. Option: Lancer Redis localement (si vous voulez tester le cache)

```bash
# Avec Docker
docker run -d -p 6379:6379 --name redis redis:alpine

# VÃ©rifier que Redis fonctionne
docker ps
```

**Alternative sans Docker**: Utiliser [Upstash](https://upstash.com) (gratuit, pas de Docker requis)

### 5. Relancer le serveur

```bash
npm run dev
```

### 6. Tester les nouveaux endpoints

```bash
# Health check
curl http://localhost:3000/api/system/health

# Devrait retourner:
# {
#   "status": "healthy",
#   "services": {
#     "database": { "status": "up" },
#     "cache": { "status": "up" ou "disabled" },
#     "llm": { "openai": "up", "anthropic": "up" ou "down" }
#   }
# }
```

---

## ğŸ“ Fichiers CrÃ©Ã©s

### Services Core
- âœ… `lib/cache/redis-cache.ts` - Service de cache Redis
- âœ… `lib/llm/unified-llm.ts` - Service LLM multi-provider
- âœ… `sentry.client.config.ts` - Config Sentry browser
- âœ… `sentry.server.config.ts` - Config Sentry server
- âœ… `sentry.edge.config.ts` - Config Sentry edge

### API Routes
- âœ… `app/api/chat/stream/route.ts` - Streaming SSE
- âœ… `app/api/system/health/route.ts` - Health check

### CI/CD
- âœ… `.github/workflows/ci.yml` - Pipeline GitHub Actions

### Tests
- âœ… `__tests__/lib/cache/redis-cache.test.ts`
- âœ… `__tests__/lib/llm/unified-llm.test.ts`
- âœ… `__tests__/lib/agent/analyst-agent.test.ts`

### Documentation
- âœ… `IMPROVEMENTS.md` - Documentation complÃ¨te
- âœ… `INSTALLATION.md` - Ce fichier

### Agents Mis Ã  Jour
- âœ… `lib/agent/analyst-agent.ts` - Utilise nouveau service LLM
- âœ… `lib/agent/reader-agent.ts` - Utilise nouveau service LLM
- âœ… `lib/env.ts` - Variables d'environnement ajoutÃ©es

---

## âœ… VÃ©rification Post-Installation

### 1. VÃ©rifier que les packages sont installÃ©s

```bash
npm list @sentry/nextjs @anthropic-ai/sdk ai
```

### 2. Tester la compilation TypeScript

```bash
npx tsc --noEmit
```

### 3. Lancer les tests

```bash
npm test
```

### 4. Tester un brief avec le nouveau systÃ¨me

1. Aller sur http://localhost:3000
2. CrÃ©er un brief
3. VÃ©rifier dans la console serveur:
   - âœ… `Cache hit` ou `Cache miss` messages
   - âœ… Provider utilisÃ© (OpenAI ou Anthropic si configurÃ©)
   - âœ… CoÃ»t par appel LLM

---

## ğŸ¯ BÃ©nÃ©fices ImmÃ©diats

### Avec Redis configurÃ©:
- ğŸ’° **-80% coÃ»ts** sur requÃªtes rÃ©pÃ©tÃ©es
- âš¡ **-90% latence** grÃ¢ce au cache

### Avec Anthropic configurÃ©:
- ğŸ›¡ï¸ **+0.4% uptime** (fallback automatique)
- ğŸ¯ **Meilleure qualitÃ©** analyse (Claude 3.5 Sonnet)

### Avec Sentry configurÃ©:
- ğŸ› **Zero guessing** en production (stack traces complets)
- ğŸ“Š **Alertes temps rÃ©el** sur erreurs critiques

---

## ğŸ”§ Troubleshooting

### "Module not found: @sentry/nextjs"
```bash
# RÃ©installer
npm install @sentry/nextjs --force
```

### Redis connection failed
```bash
# VÃ©rifier que Redis tourne
docker ps | grep redis

# Si pas lancÃ©
docker start redis

# Ou lancer nouveau container
docker run -d -p 6379:6379 --name redis redis:alpine
```

### "Cannot find module '@/lib/llm/unified-llm'"
```bash
# Reconstruire
npm run build
# Ou juste dev
npm run dev
```

### Tests Ã©chouent
```bash
# GÃ©nÃ©rer Prisma client d'abord
npm run prisma:gen

# Puis tests
npm test
```

---

## ğŸ“Š Prochaines Ã‰tapes RecommandÃ©es

### ImmÃ©diat (aujourd'hui)
1. âœ… Configurer Redis (Upstash = 5 min)
2. âœ… Tester un brief pour voir le cache fonctionner
3. âœ… VÃ©rifier `/api/system/health`

### Cette semaine
1. âš ï¸ CrÃ©er compte Sentry (gratuit)
2. âš ï¸ CrÃ©er compte Anthropic (fallback)
3. âš ï¸ Migrer autres agents vers nouveau service LLM

### Ce mois
1. ğŸ¯ Configurer GitHub Actions (secrets)
2. ğŸ¯ DÃ©ployer sur Vercel avec variables env
3. ğŸ¯ Monitorer mÃ©triques (coÃ»ts, cache hit rate)

---

## ğŸ†˜ Besoin d'aide ?

- ğŸ“– Documentation: Voir `IMPROVEMENTS.md`
- ğŸ› Bugs: VÃ©rifier logs Sentry (si configurÃ©)
- âš¡ Performance: Check `/api/system/health`
- ğŸ’¬ Questions: GitHub Issues

---

**Version**: 2.0.0  
**Date**: 2026-01-23  
**Status**: PrÃªt pour installation manuelle
